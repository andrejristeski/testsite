<!DOCTYPE html>
<html> 
<head>
  <!--link rel="stylesheet" type="text/css" href="style.css"-->
  <link rel="stylesheet" type="text/css" href="style.css">
  <meta charset="utf-8"> 

<title>Andrej Risteski</title>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-27318059-2', 'auto');
  ga('send', 'pageview');

</script>


</head>
<body>



<div class="banner">
<div class="picture"><h2><strong>Andrej Risteski</strong> </h2>  </div>
<div class="logo"> <img src="scslogo_no_outline_simple.gif" height="60px"/>     </div>
</div>


<div style="width: 70%;">
    <div style="padding-left: 2rem; padding-right: 2rem; padding-bottom: 2rem; padding-top: 2rem;">
<table class="imgtable"><tr><td>
<a href="photo_cmu.png"><img src="photo_math.png" alt="Andrej Risteski" height="317px"/></a>&nbsp;</td>
<td align="left">



<p> I am an Assistant Professor at the <a href="https://www.ml.cmu.edu/"> Machine Learning Department</a> in <a href="https://www.cmu.edu/"> Carnegie Mellon University</a>.   
<!-- <p> I hold a joint position as the Norbert Wiener fellow at the <a href="https://idss.mit.edu">Institute for Data Science and Statistics (IDSS)</a> and an instructor of <a href="https://math.mit.edu/research/applied/index.php">Applied Mathematics</a> at MIT.  </p> -->
I received my PhD in the <a href="http://www.cs.princeton.edu">Computer Science Department</a> at Princeton University under the advisement of <a  href="https://www.cs.princeton.edu/~arora">Sanjeev Arora</a>.  </p> 

 <p> My research interests lie in machine learning and statistics, spanning topics like representation learning, generative models, word embeddings, variational inference and MCMC and non-convex optimization.</p>

<p>
The broad goal of my research is principled and mathematical understanding of statistical and algorithmic phenomena and prob-
lems arising in modern machine learning.
 </p>

<!--The mathematical tools I like to use often come from areas like statistics, theoretical computer science and statistical physics. </p>-->
 
<!-- <p> 
Read my <a href="cv.pdf"> CV </a> for more details, or browse my publications below.  
</p> -->

</td></tr></table>
   </div> 
</div>


 <h3><strong> Papers  </strong></h3> <br>
 <h4> (In alphabetical order, following the tradition in theoretical computer science) </h4><br><br><br>   

<h4><strong> Understanding representations in supervised and unsupervised learning </strong></h4> 
<ul>
<li> Theoretical Analysis of the Representational Power of GANs and Flow Models. With Frederic Koehler and Viraj Mehta. <i> Manuscript 2020.</i> </li> 
<li> On Learning Language-Invariant Representations for Universal Machine Translation. With Han Zhao, Junjie Hu. <i> ICML 2020. </i>  </li>  
<li> <a href="https://arxiv.org/abs/1907.00030"> Benefits of Overparameterization in Single-Layer Latent Variable Generative Models. </a> With Rares Buhai, Yoni Halpern and David Sontag. <i> ICML 2020. </i>  </li>  
<li> <a href="https://openreview.net/forum?id=rJfW5oA5KQ"> Approximability of Discriminators Implies Diversity in GANs. </a> With Yu Bai and Tengyu Ma. <i> ICLR 2019. </i>  </li>
<li> <a href="https://openreview.net/forum?id=rJgTTjA9tX&noteId=ByeUreMZhX"> Representational Power of ReLU Networks and Polynomial Kernels: Beyond Worst-Case Analysis. </a> With Frederic Koehler. <i> ICLR 2019. </i>  </li>
<li> <a href="https://openreview.net/forum?id=BJehNfW0-"> Do GANs learn the distribution? Some theory and empirics.  </a> With Sanjeev Arora and Yi Zhang. <i> ICLR 2018 </i>  </li>
<li> <a href="http://arxiv.org/abs/1601.03764"> Linear algebraic structure of word senses, with applications to polysemy. </a> With Sanjeev Arora, Yuanzhi Li, Yingyu Liang and Tengyu Ma. <i>  Transactions of the Association for Computat
ional Linguistics (TACL), 2018 </i> </li>
<li> <a href="http://www.aclweb.org/anthology/W17-1902 "> Automated WordNet Construction Using Word Embeddings. </a> With Mikhail Khodak, Christiane Fellbaum, Sanjeev Arora. <i> EACL Workshop on Sense, Concept and Entity Representation
    s and their Applications, 2017</i> </li>
<li> <a href="https://arxiv.org/pdf/1702.07028.pdf"> On the ability of neural nets to express distributions. </a> With Holden Lee, Rong Ge, Tengyu Ma, Sanjeev Arora. <i> COLT 2017 </i> </li>
<li> <a href="http://arxiv.org/abs/1502.03520"> RAND-WALK: a latent variable model approach to word embeddings. </a> With Sanjeev Arora, Yuanzhi Li, Yingyu Liang and Tengyu Ma. <i> Transactions of the Association for Computational Linguistics (TACL), 2016 </i>  </li>
<!-- <li> <a href="https://arxiv.org/abs/1706.04601"> Provable benefits of representation learning. </a> With Sanjeev Arora. <i> Manuscript </i> </li>
<li> <a href="https://arxiv.org/abs/1711.02651"> Theoretical limitations of Encoder-Decoder GAN architectures.  </a> With Sanjeev Arora and Yi Zhang. <i> Manuscript </i> </li> -->
</ul>

<h4><strong> Provable algorithms for learning and inference in probabilistic and generative models </strong> </h4> 
<ul>
    <li> <a href="https://arxiv.org/abs/2002.05576"> Fast Convergence for Langevin Diffusion with Matrix Manifold Structure.</a> With Ankur Moitra.  <i> Manuscript  2020</i>  </li>
  <li> <a href="https://arxiv.org/abs/1710.02736"> Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo. </a> With Rong Ge and Holden Lee.  <i> NeurIPS 2018  </i>  </li>
 <li> <a href=" https://arxiv.org/abs/1808.07226"> Mean-field approximation, convex hierarchies, and the optimality of correlation rounding: a unified perspective. </a> With Vishesh Jain and Frederik Koehler. <i> STOC 2019. </i> </li>
<li> <a href="https://arxiv.org/abs/1612.08795"> Provable learning of noisy-or networks. </a> With Sanjeev Arora, Rong Ge, and Tengyu Ma. <i> STOC 2017 </i> </li>
<li> <a href="http://arxiv.org/abs/1607.03183">  How to calculate partition functions using convex programming hierarchies: provable bounds for variational methods.</a>  <i> COLT 2016, long talk </i> </li>
<li> <a href="http://arxiv.org/abs/1607.03360"> Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods. </a> With Yuanzhi Li. <i> NeurIPS 2016 </i> </li>
<li> <a href="https://arxiv.org/abs/1611.03819"> Non-negative matrix factorization using a decode-and-update approach. </a> With Yuanzhi Li and Yingyu Liang.  <i> NeurIPS 2016 </i>  </li>
<li><a href="http://arxiv.org/abs/1602.02262"> Recovery guarantee of weighted low-rank approximation via alternating minimization. </a> With Yuanzhi Li and Yingyu Liang. <i> ICML 2016  </i> </li>
<li> <a href="http://arxiv.org/abs/1503.06567"> On some provably correct cases of variational inference for topic models. </a> With Pranjal Awasthi. <i> NeurIPS 2015, Spotlight </i> </li>
<!-- <li>  New practical algorithms for learning noisy-or networks via symmetric NMF. With Sanjeev Arora, Rong Ge, and Tengyu Ma. <i> Manuscript </i> </li> -->
</ul>

<h4><strong>  Computational issues around online and improper learning  </strong> </h4>
<ul>
  <li> <a href="https://arxiv.org/abs/1905.13283">  Sum-of-squares meets square loss: Fast rates for agnostic tensor completion. </a> With Dylan J. Foster. <i> COLT 2019 </i> </li>   
<!--  <li> <a href="sparse_regression_arxiv.pdf"> Computational hardness of fast rates for online sparse PCA: improperness does not help. </a> With Elad Hazan. <i> Manuscript 2018. </i>  </li> -->
  <li> <a href="https://papers.nips.cc/paper/6576-algorithms-and-matching-lower-bounds-for-approximately-convex-optimization.pdf"> Tight algorithms and lower bounds for approximately convex optimization. </a> With Yuanzhi Li.  <i> NeurI
PS 2016 </i>  </li>
     <li> <a href="http://arxiv.org/abs/1503.02193"> Label optimal regret bounds for online local learning. </a> With Pranjal Awasthi, Moses Charikar and Kevin A. Lai. <i> COLT 2015 </i>  </li>
</ul> 
<!--<h4><strong> Other topics </strong> </h4> 
<ul>
    <li><a href="cccg.pdf"> What makes a tree a straight skeleton? </a> With Oswin Aichholzer, Howard Cheng, Thomas Hackl, Stefan Huber, Brian Li. <i> Canadian Conference on Computational Geometry 2012. </i>   </li>
    <li><a href="http://arxiv.org/abs/1203.5782"> Skeletal rigidity of phylogenetic trees. </a> With Howard Cheng, Satyan Devadoss, Brian Li. <i> Discrete Applied Mathematics 170, 2014. </i> </a> </li>
    <li> <a href="http://arxiv.org/abs/1512.01829"> On routing disjoint paths in bounded treewidth graphs. </a> With Alina Ene, Matthias Mnich and  Marcin Pilipczuk. <i> To appear in SWAT 2016 </i> </li>
    <li> <a href="https://arxiv.org/pdf/1702.07028.pdf"> On the ability of neural nets to express distributions.* </a> With Holden Lee, Rong Ge, Tengyu Ma, Sanjeev Arora. <i> To appear in COLT 2017 </i> </li>

 </ul>  -->
 
<h3><strong> Talks </strong></h3>
<ul>
  <li><font color="#003153">  Diffusing along manifolds of local optima via Langevin dynamics </font></li>
   <ul> <li> Microsoft Research New England, 03/19 </li>
   <li> MIFODS Workshop on Learning with Complex Structure, MIT, 01/20 </li> 
   </ul>
  <li><font color="#003153"> Mean-field approximation and variational methods via convex relaxations </font></li>
  <ul> <li> Harvard Physics and Computation Seminar, 10/18 </li>
  <li> MIT Seminar on Stochastic Processes, 11/18 </li>  
   </ul>
  <li><font color="#003153">  Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo </font></li>
   <ul> <li> MIT Algorithms and Complexity Seminar, 11/01/17 </li>
   </ul>
  <li> <font color="#003153"> Provable algorithms for learning noisy-OR networks  </font> </li>
    <ul>  <li> STOC (Montreal, 2017) </li>  </ul>
 
  <li> <font color="#003153"> Theoretical aspects of representation learning </font> </li>
    <ul>  <li> Simons Institute for the Theory of Computing, 03/27/17 </li>  </ul>
      
 <li> <font color="#003153"> New techniques for learning and inference in probabilistic graphical models </font> </li>
 <ul>       <li> MIT Stochastics and Statistics Seminar, 09/08/17 </li>
   <li> Microsoft Research Redmond, 02/08/17 </li>  </ul>


 <li>  <font color="#003153"> How to calculate partition functions using convex programming hierarchies: provable bounds for variational methods </font></li>
     <ul>
        <li> Stanford Theory Seminar, 02/02/17 </li>        
        <li> Los Alamos National Laboratory, 11/07/16 </li>
        <li> Rutgers University, 10/19/16 </li>     
        <li> COLT (New York City, 2016)  <a href="https://www.youtube.com/watch?v=b2cqttSJorQ"> [Video] </a> </li>
      </ul>  
       <li>  <font color="#003153"> On some provably correct cases of variational inference for topic models </font></li>
     <ul>
       <li> NeurIPS (Montreal, 2015) <a href="http://research.microsoft.com/apps/video/?id=259590"> [Video, talk starts circa 11:45] </a>  </li>
     </ul>
  <li>  <font color="#003153"> Random walks on context spaces: towards an explanation of the mysteries of semantic word embeddings </font></li>
     <ul> 
       <li> China Theory Week (Jiao Tong University, Shanghai, 2015) </li>
     </ul>
   <li> <font color="#003153"> Label optimal regret bounds for online local learning </font></li> 
     <ul> 
        <li> COLT (Paris, 2015) <a href="http://videolectures.net/colt2015_risteski_local_learning/"> [Video] </a> </li>
     </ul>     
   
</ul>

<!-- <h3><strong> Awards </strong></h3> 
<ul>
 <li> School of Engineering and Applied Science Award for Excellence, 2016 </li>
 <li> Princeton Honorific Fellowship nominee, 2016 </li>
 <li> Member of Phi Beta Kappa, Tau Beta Pi, Sigma Xi</li>
 <li> Shapiro Prize for freshman year at Princeton University - 2009 </li>
 <li> Bronze medal at Balkan Mathematical Olympiad – 2007 </li>
 <li> Honorable Mention at International Mathematical Olympiad – 2007 </li> 
</ul>  -->

<h3><strong> Blog posts about my work  </strong></h3> <br>
<ul>
<li><a href="https://windowsontheory.org/2018/11/11/approximating-partition-functions/"> On approximating partition functions via variational methods. </a> </li>    
<li> <a href="http://www.offconvex.org/2018/03/12/bigan/"> Theoretical limitations of modern GAN architectures.  </a> </li>
<li> <a href="http://www.offconvex.org/2017/06/26/unsupervised1/"> Formalizations of representation learning. </a> </li>
<li> <a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/"> Word embeddings.  </a> </li>
</ul>


<h3><strong> Teaching </strong></h3> 
<ul>
<li> Instructor for > <a href="https://andrejristeski.github.io/10707-S20/syllabus.html"> 10.707 (Advanced Deep Learning)</a> at CMU: Spring 2020 </li>
   <li>Instructor for 18.200A (Principles of Discrete and Applied Mathematics) at MIT: Fall 2017/18 and Fall 2018/19</li>
</ul>

<h3><strong> Contact </strong></h3> 

<ul> <li>The easiest way to reach me is email. My address is aristesk <i>at</i> andrew.cmu.edu  </li> </ul>


</body>


</html>
